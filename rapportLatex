% !TEX program = xelatex % (can also use "lualatex") 
\documentclass[12pt,a4paper]{report}

% --- Preamble ---
%\usepackage{fontspec}
\usepackage{polyglossia}
% --- Scrum visuals & tables ---
\usepackage{float}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

\setmainlanguage{english}
\setotherlanguage{arabic}

\setmainfont{Times New Roman} % Or any other suitable font for English
\newfontfamily\arabicfont[Script=Arabic]{Amiri}

\usepackage{geometry}
\geometry{margin=1in}
\usepackage{amsmath}

\usepackage{graphicx}   % needed for \includegraphics
\usepackage{pdfpages}   % needed for \includepdf
\usepackage{caption}
\captionsetup{style=default}
\usepackage[hidelinks]{hyperref}
\usepackage[nameinlink]{cleveref}
\usepackage{appendix}
\usepackage{listings}

\lstset{
    basicstyle=\small\ttfamily,
    breaklines=true,
    frame=single,
    showstringspaces=false,
    language=rust
}

% ---------------- Document ----------------
\begin{document}
% path to your images folder
\graphicspath{{figures/}}


% Roman numbering for front matter
\pagenumbering{roman}

% ---- ToC ----
\tableofcontents
\listoffigures
\listoftables
\newpage

% Switch to arabic numbering for main matter
\pagenumbering{arabic}

% ================================
% General Introduction
% ================================
\chapter*{General Introduction}
\addcontentsline{toc}{chapter}{General Introduction}

This thesis, undertaken for the *Ministère de l'Intérieur*, addresses these challenges by designing and implementing an intelligent system for the robust matching of Arabic biographical records. We propose a hybrid pipeline that combines rule-based text normalization to unify orthographic variants, a custom phonetic algorithm called Aramix Soundex to account for similarities in pronunciation, and weighted fuzzy string matching using Levenshtein and Jaro-Winkler distances to accurately rank potential matches.

This high-performance engine, built in Rust and served via a REST API, powers an intuitive Angular web application that empowers government agents to find individuals quickly and reliably. Furthermore, the system leverages a Neo4j graph database to model family trees, allowing for the reconstruction of familial relationships from identity records. The visualization of these trees is handled by an automated n8n workflow, which processes the graph data, uses a generative AI to create a diagram, and delivers it to the user.

By bridging this critical gap, our work aims not only to solve a technical problem but also to restore dignity and access for countless individuals. The following chapters will detail the design, implementation, and evaluation of this solution, demonstrating a practical path toward a more inclusive and efficient digital state.


% ================================
% Chapter 1
% ================================
\chapter{Presentation of the Host Organization}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.6\textwidth]{figures/inetum.jpg}
  \caption{Inetum, the host organization.}
  \label{fig:inetum-intro}
\end{figure}

\section{Inetum: Global Overview}
Inetum, formerly known as GFI (Groupe Français d'Informatique), is a global IT services company specializing in digital transformation and consulting. In 2021, the group rebranded to Inetum to mark a new era as an expert in the "digital flow."

Today, Inetum operates in over 27 countries across Europe, the Americas, Asia, and Africa. With a dedicated team of nearly 27,000 professionals, the company generated a revenue of €2.2 billion in the last fiscal year. It provides a wide range of services, from consulting and innovation to application development, leveraging its local expertise on a global scale.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/Key_figures_of_Inetum.png}
  \caption{Key figures of Inetum (employees, offices, revenue, etc.).}
  \label{fig:key-figures-inetum}
\end{figure}

\section{Inetum in the World}
Inetum maintains strong alliances with the world's leading software and hardware vendors to deliver robust and integrated solutions. Key strategic partners include SAP, Microsoft, IBM, Oracle, Salesforce, and Sage. These partnerships enable Inetum to offer cutting-edge technologies and enterprise-grade solutions to its clients.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/Inetum_Worldwide.png} 
  \caption{Inetum worldwide presence.}
  \label{fig:inetum-worldwide}
\end{figure}

\section{Inetum in Tunisia}
The Tunisian subsidiary of Inetum has over a decade of experience in IT organization and systems integration. With a team of more than 250 employees, the branch has achieved an annual revenue of €10 million in recent years.

Inetum Tunisia serves both public and private sector clients by leveraging the group's global expertise while adapting it to the regional context. The branch's activities are structured around four main pillars:
\begin{itemize}
    \item \textbf{Consulting and Services:} Including custom development for web and mobile portals, IoT solutions, and UX design.
    \item \textbf{Business Intelligence:} Providing data-driven insights and analytics.
    \item \textbf{Enterprise Solutions:} Implementing and managing solutions from partners like SAP and Microsoft.
    \item \textbf{Human Resources (SIRH):} Offering specialized HR information systems.
\end{itemize}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/org_chart.png}
  \caption{High-level organizational chart (Inetum Tunisia).}
  \label{fig:org-chart}
\end{figure}






\section{Core Values}
Inetum's core values are ambition, commitment, excellence, innovation, and solidarity. These values guide the company's actions and decisions, and they are reflected in the quality of the services and solutions that Inetum provides to its clients.



\section{Clients}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.9\textwidth]{figures/Clients.jpg}
  \caption{Representative Inetum clients across sectors.}
  \label{fig:inetum-clients}
\end{figure}

\subsection{Sectors of Activity}
With its multi-specialist profile, Inetum provides a distinctive blend of proximity, sector-based organization, and high-quality industrial solutions to its clients. The group serves a wide range of industries, including:
\begin{itemize}
    \item Banking and Financial Services
    \item Telecommunications and Technology
    \item Retail and Services
    \item Government and Public Sector
    \item Manufacturing and Industry
\end{itemize}

\subsection{Strategic Partners}
Inetum maintains strong alliances with the world's leading software and hardware vendors to deliver robust and integrated solutions. Key strategic partners include SAP, Microsoft, IBM, Oracle, Salesforce, and Sage. These partnerships enable Inetum to offer cutting-edge technologies and enterprise-grade solutions to its clients.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=.9\textwidth]{figures/partners.png}
  \caption{Strategic partners supporting Inetum’s services.}
  \label{fig:inetum-partners}
\end{figure}

\section{Conclusion}
This chapter has provided a detailed introduction to the host organization, Inetum, a global leader in IT services, and its strategically important Tunisian branch. By presenting an overview of its worldwide operations, core values, key sectors of activity, and strategic partnerships, we have established the corporate context in which this project was developed. Understanding the professional environment and the high standards of a company like Inetum is crucial, as it sets the stage for the quality and ambition expected in the project that follows. This foundational knowledge of the organization provides a backdrop for the technical challenges and solutions that will be discussed in the subsequent chapters.


% ================================
% Chapter 2
% ================================
\chapter{Project Context and Planning}

\section{Introduction}
This chapter transitions from the organizational context to the core of the project itself. The primary goal of this initiative is to replace an outdated, manual, and often inefficient identity verification system with a modern, high-performance, and intelligent solution. The existing processes, heavily reliant on paper-based records and basic digital tools, are inadequate for handling the linguistic nuances of Arabic names, leading to significant delays and potential errors that impact citizen services.

This chapter will provide a comprehensive overview of the project, beginning with a detailed problem statement that highlights the specific challenges faced by the Ministère de l'Intérieur. From there, we will define the project's objectives and scope, outlining what we aim to achieve. We will then present the proposed solution, including its high-level architecture and the technology stack chosen to meet the demanding requirements of performance, accuracy, and scalability. Finally, the chapter will cover the development methodologies and project planning that guided the execution of this complex undertaking, ensuring a structured path from conception to deployment.

\section{Problem Statement}
The motivation for this project stems from the shortcomings of the current systems, which are ill-equipped to handle the complexities of Arabic names. The core challenges can be summarized as follows:
\begin{itemize}
    \item \textbf{Linguistic Variability of Arabic Names:} Orthographic and phonetic ambiguities create significant challenges for digital systems that rely on exact matches.
    \item \textbf{Costs and Risks of Manual Verification:} The manual process is slow, expensive, and prone to errors that can deny citizens access to essential services.
    \item \textbf{Need for a Modern and Responsive UX:} The current tools are outdated and inefficient, leading to a poor user experience for government agents and long wait times for citizens.
\end{itemize}

The problem is particularly acute for individuals who may lack a national ID number, including:
\begin{itemize}
    \item \textbf{Newborns and Unregistered Births:} Especially in rural areas where births may not be officially declared in a timely manner.
    \item \textbf{Minors:} Who often rely on family booklets or birth certificates before being issued a national ID at age 16 or 18.
    \item \textbf{Citizens Living Abroad:} Who may have lost documents or only have foreign transliterations of their names.
    \item \textbf{Elderly People:} Many of whom were born before the era of digitization and are only listed in handwritten registers.
    \item \textbf{Vulnerable Populations:} Such as homeless individuals or refugees who may be disconnected from the civil registry.
\end{itemize}

\section{Study of the Existing System}
The current identity verification process is predominantly manual, leading to significant inefficiencies and risks. Key characteristics of the existing system include:
\begin{itemize}
    \item \textbf{Manual Paper-Based Management:} All requests and supporting documents are archived on printed media. Biographical information is often entered by hand into physical registers or basic spreadsheets.
    \item \textbf{Visual Matching and Spreadsheets:} Agents manually scan lists to identify duplicates and inconsistencies. This process relies heavily on intensive filtering and sorting in tools like Excel or Access, which lack sophisticated similarity algorithms.
    \item \textbf{Lack of Centralization and Automation:} Data is fragmented across silos in different departments, consulates, and local databases. The process is slow, prone to human error, and provides no real-time feedback.
\end{itemize}

\section{Objectives \& Success Criteria}
The primary objective of this project is to develop a highly accurate and efficient system for Arabic identity matching.
Success will be measured by the following criteria:
\begin{itemize}
    \item \textbf{Accuracy:} The system must achieve a high precision and recall rate in matching Arabic names.
    \item \textbf{Performance:} The matching process should be fast enough to handle real-time requests.
    \item \textbf{Scalability:} The system should be able to handle a large volume of data and users.
    \item \textbf{Usability:} The user interface should be intuitive and easy to use.
\end{itemize}

\section{Project Scope}
\subsection{Scope, Dataset, and Boundaries}
The scope of this project is to develop a system for matching Arabic identities from a given dataset. The dataset consists of a large number of identity records, each containing personal information such as name, date of birth, and place of birth. The system assumes that the input data is in a specific format and that the database is accessible. The main constraint is the performance of the matching algorithm, which must be fast enough to handle real-time requests.

\subsection{Stakeholders \& Core Use Cases}
The main stakeholders of the project are the users who need to match identities (e.g., government agents), and the administrators who manage the system. The core use cases include:
\begin{itemize}
    \item Searching for a citizen's record using partial or inexact information.
    \item Verifying an individual's identity when a national ID is not available.
    \item Reconstructing family relationships through the family tree visualization.
\end{itemize}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=.9\textwidth]
  {figures/Core Use Cases Hierarchy.png}
  \caption{Core Use Cases Hierarchy.}
  \label{fig:Core_Us_Cases_Hierarchy}
\end{figure}

\section{Proposed Solution}

\subsection{Proposed Solution ---مطابق الهوية}
The proposed solution is a modern, hybrid system that addresses the core challenges of Arabic name matching.
Hybrid identity matching combines text normalization to unify orthographic variants, a custom phonetic algorithm (Aramix Soundex), and fuzzy matching using Levenshtein and Jaro-Winkler.
Visualization and automation are achieved through n8n for process orchestration, Gemini AI for data parsing and simplification, and Python for automated diagram generation.

The technology stack includes Angular for the frontend, Rust with Axum for a high-performance backend, PostgreSQL for structured identity records, and Neo4j for managing family tree relationships.

For deployment, the entire system is containerized.

\subsection{General Overview}
This project focuses on the development of an identity matching system specifically
adapted to the Arabic language. Unlike Latin-based systems, Arabic introduces unique challenges such as orthographic variability, phonetic ambiguity, and transliteration issues.

For example, the system must handle:
\begin{itemize}
    \item \textarabic{همزة} (ء) in its multiple forms (\textarabic{أ، إ، ؤ، ئ}) and their interchangeable usage.
    \item \textarabic{تنوين} (\textarabic{ً ٍ ٌ}) which may appear or be omitted in different records.
    \item Diacritics (\textarabic{حركات}) like \textarabic{َ ِ ُ} that are often inconsistently written.
    \item Ligatures such as (\textarabic{ﻻ}) for "lam-alef".
    \item Variations in letter forms, e.g., "\textarabic{ة}" vs "\textarabic{ه}", or "\textarabic{ى}" vs "\textarabic{ي}".
    \item Prefix and naming conventions (\textarabic{الـ، ابن، بن، بنت، أبو، أم}).
\end{itemize}

The objective is to design a reliable and scalable solution that integrates seamlessly into existing organizational workflows while accounting for these Arabic-specific linguistic variations. By combining normalization, phonetic encoding (Aramix Soundex), and fuzzy similarity scoring, the system ensures robust identity matching across heterogeneous data sources.

\subsection{High-Level Architecture}
The system is designed with a microservices-oriented approach, separating the core matching logic from the family tree generation process. The main components are:
\begin{itemize}
    \item \textbf{Angular Frontend:} A responsive single-page application providing the user interface for government agents.
    \item \textbf{Rust Backend:} A high-performance REST API that handles user authentication, identity matching requests against the PostgreSQL database, and initiates family tree generation by calling the n8n workflow.
    \item \textbf{Databases:} PostgreSQL for storing primary identity records and Neo4j for modeling and querying complex family relationships as a graph.
    \item \textbf{n8n Automation Workflow:} A dedicated workflow that orchestrates the entire family tree visualization process, acting as a middleware that connects multiple services.
\end{itemize}

The family tree generation follows a specific, automated sequence, as summarized in the following speaker notes.

\subsubsection*{Family Tree Diagram Speaker Notes (short):}
\begin{quote}
“The family tree feature starts when an agent requests to generate a person’s family tree. The request goes through the Family Tree API, which checks credentials, fetches raw family data, and sends it to the n8n workflow service. AI processing transforms the raw data into structured JSON, which is returned as a tree visualization. Finally, the interface renders the complete family tree for the agent to explore.”
\end{quote}

\subsubsection*{n8n Workflow (Concise Script):}
\begin{quote}
“Webhook receives Neo4j JSON (raw relations). The payload is sent to Gemini with a prompt to parse and normalize entities/links. The structured output flows to a Python script that renders diagrams, then returns visuals to the frontend via the webhook response.”
\end{quote}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/n8nwork.png}
  \caption{n8n workflow for family tree generation.}
  \label{fig:n8n-workflow}
\end{figure}

\paragraph{LLM Choice}
Currently, Gemini serves as the AI model within the n8n workflow. For future iterations, a self-hosted LLM (such as Ollama or another open-source alternative) could be dedicated to this function to provide a more secure and faster response.

\subsection{Technology Stack}
The choice of technologies was driven by the project's requirements for performance, scalability, and modern user experience. Each component was selected to fill a specific role in the architecture.

\begin{itemize}
    \item \textbf{Rust \includegraphics[height=1.2em]{figures/Rust.png}:} Chosen for the backend due to its exceptional performance, memory safety, and concurrency features, which are critical for handling real-time matching requests efficiently.
    \item \textbf{Angular \includegraphics[height=1.2em]{figures/Angular.png}:} A powerful frontend framework used to build the responsive and feature-rich single-page application (SPA) that serves as the user interface for government agents.
    \item \textbf{PostgreSQL \includegraphics[height=1.2em]{figures/postgres.png}:} A reliable and scalable open-source relational database used to store the core identity records, offering robust data integrity and performance.
    \item \textbf{Neo4j \includegraphics[height=1.2em]{figures/Neo4j.png}:} A native graph database ideal for managing and querying the complex, interconnected data of family trees. Its graph-based model allows for efficient traversal of familial relationships.
    \item \textbf{n8n \includegraphics[height=1.2em]{figures/n8n.png}:} A low-code workflow automation tool used to orchestrate the family tree visualization process. It acts as the glue between the backend, the AI model, the Python script, and Google Drive, allowing for complex process automation without cluttering the main backend codebase.
    \item \textbf{Google Gemini API:} A powerful generative AI model used within the n8n workflow to intelligently parse the complex JSON output from Neo4j and transform it into a simplified, structured format suitable for the diagram generation script.
    \item \textbf{Python:} Used for a dedicated microservice whose sole responsibility is to take the simplified JSON data and generate a family tree diagram. This separation of concerns keeps the main backend focused on its core tasks.
   
    \item \textbf{Docker:} Used for containerizing the entire application stack (backend, frontend, databases), ensuring consistent development and deployment environments.
\end{itemize}

\subsection{Deployment Strategy}
The entire system is designed for containerized deployment using Docker, ensuring consistency across development, testing, and production environments. This approach simplifies dependency management and enhances portability.

\subsubsection{Application Containerization}
Both the Rust backend and the Angular frontend are containerized using multi-stage \textbf{Dockerfiles}. This best-practice technique involves using a first stage (the "builder") with the full SDK (Rust and Node.js, respectively) to compile the application and its dependencies. The second stage then copies only the final compiled artifact (the Rust binary and the static Angular files) into a minimal, production-ready base image (like debian:bullseye-slim or nginx:alpine). This process results in final images that are significantly smaller, have a reduced attack surface, and are free from unnecessary build tools and source code, making them more secure and efficient.

\subsubsection{Service Orchestration}
The various services are orchestrated using \textbf{Docker Compose}. The docker-compose.yml file defines and configures the application's services, networks, and volumes. It automates the process of starting the entire stack, including:
\begin{itemize}
    \item The \textbf{Angular frontend} service, built from its Dockerfile and served by Nginx.
    \item The \textbf{Rust backend} service, built from its Dockerfile.
    \item A \textbf{PostgreSQL} service for the relational database.
\end{itemize}
This setup ensures that all components can communicate with each other over a defined Docker network and that database state can be persisted across container restarts.

The \textbf{n8n workflow}, while central to the family tree generation process, is managed as a separate deployment. It can be self-hosted using its official Docker image and is integrated with the main application via secure webhook calls, a common pattern in microservice architectures.

\section{Development Methodology}
The project follows a hybrid approach, combining the Scrum framework for project management with the CRISP-DM methodology for the data science lifecycle. Scrum governed how the team organised work, inspected progress, and adapted priorities, while CRISP-DM supplied the analytical structure for the data-oriented tasks tackled during each sprint.

\subsection{Scrum Framework}

\subsubsection*{Team Roles and Responsibilities}
The Scrum Team remained compact in order to maintain short feedback loops while still covering all mandatory roles.\\
\textbf{Product Owner:} Ministère de l'Intérieur representative in charge of expressing needs, validating increments, and refining acceptance criteria.\\
\textbf{Scrum Master:} Mouaïa Ben Hamed, facilitator of ceremonies, blocker removal (e.g., gaining secured dataset access), and guardian of Scrum values.\\
\textbf{Developer:} Ilef (author), responsible for implementing increments, estimating work, preparing demonstrations, and logging improvement actions.

\subsubsection*{Sprint Cadence and Planning}
Delivery was structured around four \emph{two-week} sprints. Each iteration began with \textbf{Sprint Planning} to agree on a clear Sprint Goal, select the highest-value Product Backlog items, and decompose them into tasks sized for the available capacity (teaching duties and exams were explicitly factored in). Sprint Goals emphasised vertical slices so that every iteration produced a potentially shippable increment deployable to the staging environment.

\subsubsection*{Scrum Ceremonies and Contribution to Progress}
Daily Scrums (15 minutes) with the Scrum Master synchronised efforts, surfaced impediments such as missing Neo4j indexes, and kept focus on the Sprint Goal. Sprint Reviews showcased the increment to the Product Owner—examples include the matching dashboard in Sprint~2 and the family tree visualisation in Sprint~3—and captured feedback that immediately fed backlog reprioritisation. Sprint Retrospectives followed each review to consolidate lessons learned and decide concrete experiments (e.g., pairing backlog refinement with architecture spikes, moving Daily Scrum earlier to align agendas).

\subsubsection*{Scrum Artifacts and Backlog Evolution}
The \textbf{Product Backlog} was refined weekly. Epics were decomposed into user stories enriched with acceptance criteria, then ordered by business value, risk reduction, and technical feasibility. The \textbf{Sprint Backlog} tracked selected items and their task breakdown on a Kanban board (Todo/In Progress/Review/Done), enabling transparency during Daily Scrums. Items that failed to meet the Definition of Done (code merged, tests \(\geq\)~80\% on touched lines, documentation updated, demo scenario rehearsed, PO sign-off) were carried over intentionally, with root causes analysed in the retrospective to avoid recurring spill-overs.

\begin{table}[H]\centering
\small
\begin{tabular}{@{}llp{8.5cm}@{}}
\toprule
\textbf{Sprint} & \textbf{Goal} & \textbf{Delivered Increment (examples)}\\
\midrule
S1 & Backend foundation & Axum skeleton, initial /match endpoint, scoring v1, CI build.\\
S2 & Accuracy uplift    & PostgreSQL integration, normalization pipeline, Aramix Soundex, load test harness.\\
S3 & Usability          & Angular search dashboard, Neo4j graph queries, n8n orchestration, UX enhancements.\\
S4 & Hardening \& deploy& Golden-set validation tests, performance tuning (Rayon), Docker Compose stack, release documentation.\\
\bottomrule
\end{tabular}
\caption{Sprint summary highlighting increment-driven delivery.}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figures/Scrum schema.png}
    \caption{Scrum framework adopted for the project.}
\end{figure}

\subsection{CRISP-DM Methodology}
The data science component of the project followed the \textbf{Cross-Industry Standard Process for Data Mining (CRISP-DM)}. This methodology provides a structured approach to planning a data mining project. It is a robust and well-proven methodology. The phases were applied as follows:

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/crispdm.png}
  \caption{CRISP-DM Methodology.}
  \label{fig:crisp-dm}
\end{figure}

\begin{itemize}
    \item \textbf{Business Understanding:} Focused on defining the core problem of identity matching within the operational context of the Ministère de l'Intérieur. This involved understanding the project objectives and requirements from a business perspective.
    \item \textbf{Data Understanding:} Involved an in-depth analysis of the identity records to identify patterns, inconsistencies, and the types of orthographic and phonetic variations in Arabic names. This initial data collection and exploration helped to familiarize with the data and identify any quality issues.
    \item \textbf{Data Preparation:} Consisted of developing the text normalization pipeline to clean and standardize the names before matching. This phase covered all activities to construct the final dataset from the initial raw data.
    \item \textbf{Modeling:} Centered on designing and implementing the hybrid matching algorithm, which combines phonetic encoding (Aramix Soundex) and fuzzy string similarity metrics. Various modeling techniques were selected and applied, and their parameters were calibrated to optimal values.
    \item \textbf{Evaluation:} The model's performance was measured for accuracy using a predefined golden set of matches and for speed via load testing. Before proceeding to final deployment, it was important to thoroughly evaluate the model and review the steps executed to construct it.
    \item \textbf{Deployment:} The final system, including the backend API and frontend application, was containerized using Docker for deployment. The model was deployed into a production environment, ready to be used by the end-users.
\end{itemize}
By integrating these phases into the Scrum sprints, the development of the matching engine was both systematic and agile, ensuring that the data science components were developed iteratively and aligned with the overall project goals.

\section{Project Planning}
This section consolidates how the project roadmap was executed through Scrum. It opens with the calendar-level planning of the four sprints before recounting, sprint by sprint, the increments produced, the ceremonies that guided adaptation, and the backlog adjustments that kept priorities aligned with stakeholder value.
\subsection{Sprint Roadmap \& Timeline}
The six-month academic calendar was translated into a cadence of four production sprints lasting two weeks each, separated by short buffer periods for exams and stakeholder availability. This cadence ensured regular delivery while keeping room for inspection and adaptation.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/timeline.png}
  \caption{Sprint timeline aligned with academic milestones.}
  \label{fig:timeline}
\end{figure}

\noindent Table~\ref{tab:sprint-objectives} summarises the goal, main deliverables, and the most impactful ceremony outcomes for every sprint.

\begin{longtable}{@{}p{1.3cm}p{3.7cm}p{6cm}p{4cm}@{}}
\caption{Sprint objectives and Scrum ceremony highlights}\label{tab:sprint-objectives}\\
\toprule
\textbf{Sprint} & \textbf{Goal} & \textbf{Key Deliverables} & \textbf{Ceremony Insights}\\
\midrule
\endfirsthead
\toprule
\textbf{Sprint} & \textbf{Goal} & \textbf{Key Deliverables} & \textbf{Ceremony Insights}\\
\midrule
\endhead
S1 & Establish technical foundations & Product vision board, initial Product Backlog, authentication scaffold, architecture decision record, normalization prototype. & Review confirmed personas and prioritised search-first scope; retrospective scheduled mid-sprint backlog refinement.\\
S2 & Deliver the matching engine & Weighted scoring service, PostgreSQL integration, dataset cleansing scripts, first automated tests, performance benchmarks. & Review feedback reprioritised Neo4j integration; retrospective introduced Definition of Ready checklist.\\
S3 & Enrich user experience & Angular dashboard, Neo4j graph traversal endpoints, n8n workflow, monitoring dashboards, UX copy review. & Review highlighted need for contextual hints; retrospective moved Daily Scrum earlier to involve Product Owner.\\
S4 & Harden and deploy & Docker Compose stack, admin audit trail, pagination, release runbook, UAT fixes, knowledge transfer package. & Review approved release candidate; retrospective focused on handover actions and maintenance backlog.\\
\bottomrule
\end{longtable}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/main.rs diagram.png}
  \caption{Diagram of the main Rust application flow delivered across sprints.}
  \label{fig:main-rs-diagram}
\end{figure}

This narrative documents how each sprint translated the roadmap into working increments, detailing the backlog items selected, the design decisions taken, the implementation approach, and the feedback gathered during the ceremonies.

\subsection{Sprint 1: Backend Foundations and Matching Logic}
\paragraph{Sprint Backlog}
The backlog for this sprint included the following user stories:
\begin{itemize}
    \item Set up the Rust development environment.
    \item Create a new Axum project.
    \item Define the API endpoint for matching identities.
    \item Implement the basic data structures for identities and match results.
    \item Implement the initial version of the weighted scoring algorithm.
\end{itemize}

\paragraph{Sprint Design}
The design for this sprint focused on creating a simple and efficient API for matching identities. The API endpoint was designed to accept a JSON object with the input identity and return a JSON array of match results. The scoring algorithm was designed to be modular, so that it could be easily extended with new matching techniques in the future.

\paragraph{Implementation Details}
A single POST endpoint was created at /match. This endpoint accepts an InputIdentity object and returns a list of MatchResult objects. The implementation uses the Axum framework to handle the HTTP requests and responses. The core of the matching logic is the calculate\_full\_score function, which computes a weighted score based on the similarity of different fields. The initial implementation uses the Jaro similarity metric for string comparison.

\paragraph{Sprint Review}
The sprint review demonstrated the basic functionality of the matching API. A command-line client was used to send requests to the API and display the results. The feedback from the review was positive, and the team decided to proceed with the implementation of the more advanced matching features in the next sprint.

\subsection{Sprint 2: Database Integration and Advanced Modules}
\paragraph{Sprint Backlog}
The backlog for this sprint included the following user stories:
\begin{itemize}
    \item Load identity records from the database.
    \item Implement text normalization functions for Arabic names.
    \item Implement the Aramix Soundex phonetic algorithm.
    \item Integrate the advanced matching modules into the scoring algorithm.
\end{itemize}

\paragraph{Sprint Design}
The design for this sprint focused on improving the accuracy of the matching algorithm by adding text normalization and phonetic matching capabilities. The score\_pair\_with\_soundex function was introduced to combine string similarity with phonetic matching. The best\_score\_against\_variations function was added to handle name variations.

\paragraph{Implementation Details}
The backend was connected to the PostgreSQL database using the tokio-postgres library. A function load\_identities\_by\_generation was implemented to load records for a specific decade, which is an optimization to reduce the amount of data loaded into memory. Several normalization functions were implemented in normalization.rs to handle the complexities of Arabic text. The aramix\_soundex function was implemented in phonetic.rs to provide phonetic matching capabilities. These modules were integrated into the main scoring logic in matching.rs.

\paragraph{Sprint Review}
The sprint review demonstrated a significant improvement in the accuracy of the matching results. The system was now able to handle a wider range of variations in Arabic names. The team was confident that the system was ready for the frontend development in the next sprint.

\subsection{Sprint 3: Frontend Development and API Connection}
\paragraph{Sprint Backlog}
The backlog for this sprint included the following user stories:
\begin{itemize}
    \item Create a new Angular project.
    \item Design the user interface for the identity matching form.
    \item Implement the identity matching component.
    \item Create a service to communicate with the backend API.
    \item Display the matching results to the user.
\end{itemize}

\paragraph{Sprint Design}
The UI was designed to be simple and intuitive. A single form is used to collect the user's identity information. The matching results are displayed in a clear and concise table, with a detailed breakdown of the score for each match. The frontend is built around the IdentityMatchComponent, which is responsible for managing the user input and displaying the results. An IdentityMatchService is used to encapsulate the communication with the backend API.

\paragraph{Implementation Details}
The IdentityMatchComponent was implemented using Angular's reactive forms module to handle user input. The component subscribes to the IdentityMatchService to receive the matching results and updates the UI accordingly. The IdentityMatchService uses Angular's HttpClient to make POST requests to the backend API. The service includes interfaces for the InputIdentity and MatchResult data structures to ensure type safety.

\paragraph{Sprint Review}
The sprint review demonstrated the end-to-end functionality of the system. Users were able to enter their identity information in the web interface and see the matching results in real-time. The feedback was very positive, and the team was ready to move on to the final sprint.

\subsection{Sprint 4: Evaluation, Optimization, and Deployment}
\paragraph{Sprint Backlog}
The backlog for this sprint included the following user stories:
\begin{itemize}
    \item Create a test suite to evaluate the accuracy of the matching algorithm.
    \item Profile the performance of the backend and identify any bottlenecks.
    \item Optimize the code for performance and memory usage.
    \item Create a deployment pipeline for the system.
    \item Document the system architecture and deployment process.
\end{itemize}

\paragraph{Sprint Design}
The evaluation strategy focused on measuring the precision and recall of the matching algorithm. A "golden set" of known matches was used to test the system and identify any cases where it failed to produce the correct results. The optimization plan focused on improving the performance of the backend.

\paragraph{Implementation Details}
Performance testing was conducted using a load testing tool to simulate a large number of concurrent users. The results of the testing showed that the system was able to handle a high volume of requests without any significant degradation in performance. A deployment pipeline was created using Docker and Docker Compose to automate the deployment of the system.

\paragraph{Sprint Review}
The final sprint review demonstrated the completed system, including the performance improvements and the automated deployment pipeline. The stakeholders were very impressed with the results and approved the system for production deployment.

\subsection{Project Backlog}
\noindent The Product Backlog is ordered and refined each sprint. Table \ref{tab:pb} shows representative items and the sprint where they were delivered.

\begin{longtable}{@{}p{3cm}lp{7cm}l@{}}
\caption{Product Backlog with sprint mapping}\label{tab:pb}\\
\toprule
\textbf{Epic} & \textbf{Type} & \textbf{Description} & \textbf{Sprint}\\
\midrule
\endfirsthead
\toprule
\textbf{Epic} & \textbf{Type} & \textbf{Description} & \textbf{Sprint}\\
\midrule
\endhead
Admin Management & User Story & As an Administrator, create/edit/delete users. & S1\\
User Management  & User Story & As a User, log in/out to access features. & S3\\
Admin Management & User Story & Audit dashboard for actions/events. & S4\\
Core Matching    & User Story & Agent inputs biographical details to search matches. & S1\\
Family Tree Viz. & User Story & Click a match to view the family tree. & S3\\
Family Tree Viz. & Task       & Build n8n workflow to query Neo4j and return image. & S3\\
System Infra \& Deploy & Task & Provision PostgreSQL schema and tables. & S2\\
System Infra \& Deploy & Task & Dockerize Rust and Angular apps. & S4\\
System Infra \& Deploy & Task & Compose file for local stack. & S4\\
\bottomrule
\end{longtable}

\paragraph{Backlog Evolution}
Sprint Reviews systematically generated new backlog entries: database indexing (S2), API pagination and contextual UI hints (S3), deployment observability tasks (S4). Conversely, low-value cosmetic requests were transparently parked in the release backlog after alignment with the Product Owner, demonstrating active scope management.


\subsection{Risk \& Mitigation}
\begin{enumerate}
    \item \textbf{Risk:} The matching algorithm is not accurate enough. \textbf{Mitigation:} Use a combination of different matching techniques and a weighted scoring model.
    \item \textbf{Risk:} The performance of the system is not good enough. \textbf{Mitigation:} Use a fast programming language like Rust and parallelize the scoring of candidates.
    \item \textbf{Risk:} The system is not scalable. \textbf{Mitigation:} Use a scalable database like PostgreSQL and a stateless backend architecture.
    \item \textbf{Risk:} The user interface is not user-friendly. \textbf{Mitigation:} Use a modern frontend framework like Angular and follow best practices for UI/UX design.
    \item \textbf{Risk:} The project is not completed on time. \textbf{Mitigation:} Use an Agile development methodology and a realistic project roadmap.
\end{enumerate}

\section{Conclusion}
This chapter has provided a comprehensive presentation of the project, from its objectives to its technical and methodological choices. It establishes a clear foundation for the subsequent chapters, which will delve into the specific details of the system's analysis, design, and implementation.

% ================================
% Chapter 3
% ================================
\chapter{Analysis \& Requirements}

\section{Introduction}
Following the project's contextualization, this chapter transitions from high-level objectives to a detailed analysis of the system's required capabilities. The goal is to translate the needs of the stakeholders—government agents and administrators—into a concrete set of functional and non-functional requirements. This process begins with a formal modeling of the system's interactions through use cases, which provide a clear picture of who will use the system and for what purpose. By defining these interactions, we can systematically derive the specific features, constraints, and quality attributes that the final product must possess to be considered successful. This chapter serves as the blueprint for the subsequent design and implementation phases, ensuring that the developed solution is precisely aligned with the problem it aims to solve.

\section{Use–Case Model}
The system has two main actors: government agents and administrators.
Agents can search for matches and visualize family trees. Administrators manage users and view audit logs.
All actions are tied to credential checks, ensuring only authorized users can access the system.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/use case diagram.png}
    \caption{Use case Diagram}
    \label{fig:use-case-diagram}
\end{figure}

\subsection{Main Use Cases}
The main use cases of the system are:
\begin{itemize}
    \item \textbf{UC-01: Search for Matches:} An agent enters a citizen's details to find potential matches in the national registry.
    \item \textbf{UC-02: Reconstruct Identity Profile:} An agent uses partial information (e.g., name and parents' names) to reconstruct and verify an identity profile for a citizen with lost or missing documents.
    \item \textbf{UC-03: Visualize Family Tree:} An agent visualizes a citizen's family tree to understand familial relationships.
\end{itemize}

\subsection{Actors}
Two main actors interact with the system:
\begin{itemize}
    \item \textbf{Government agent:} verifies identities and explores family trees.
    \item \textbf{Administrator:} manages the system and oversees operations.
\end{itemize}

\subsection{ Search for Matches)}
In this flow, the Government Agent starts by entering the individual information through the user interface form and submitting the request, which forwards the authentication token to the Authentication Service. Once the credentials are validated and confirmed, the Matching Service is called to perform the search. The Matching Service queries the database to fetch all identities that match the individual's info, retrieves the data, and processes it using the matching algorithm.

After processing, the Matching Service returns a list of potential matches, which are then sent to the User Interface. Finally, the Government Agent sees the matches displayed on their screen.

\section{Requirements}
The system requirements are divided into two categories: functional and non-functional.

\subsection{Functional Requirements}
On the functional side, the system must support:
\begin{itemize}
    \item User authentication and authorization.
    \item The management of identities.
    \item A matching algorithm capable of delivering reliable accuracy scores.
    \item Family tree visualization.
    \item Audit logs of user actions for accountability.
\end{itemize}

\subsection{Non-Functional Requirements}
On the non-functional side, the solution is required to achieve:
\begin{itemize}
    \item High accuracy and precision (using a multi-stage matching algorithm).
    \item Scalability to handle millions of records.
    \item Strong security (using JWT Token for authentication and authorization).
    \item A reliable and usable user experience (with a user friendly UI that is easy to use).
\end{itemize}

\section{Validation Rules (selected)}
The following validation rules are applied to the input data:
\begin{itemize}
    \item The first and last names are required.
    \item The date of birth must be a valid date.
    \item The sex must be either male or female.
\end{itemize}


\section{Conclusion}
By detailing the use cases and deriving a clear set of functional and non-functional requirements, this chapter has established a solid foundation for the subsequent phases of the project. The requirements outlined here, from user authentication and matching accuracy to security and scalability, serve as a definitive checklist against which the final system will be measured. With this analytical groundwork in place, we have a clear and unambiguous understanding of what needs to be built. The next chapter will build directly upon these requirements to propose a technical design and architecture capable of meeting these specifications.

% ================================
% Chapter 4
% ================================
\chapter{State of the Art, Foundations, and Matching Algorithm}

\section{Introduction}
This chapter provides a thorough exploration of the academic and technical foundations underpinning the field of identity matching, with a specialized focus on the complex domain of Arabic names. Effective identity resolution is a cornerstone of modern data management, but the unique linguistic properties of the Arabic language introduce significant challenges that standard algorithms often fail to address. This chapter will dissect these challenges, survey the existing state-of-the-art solutions, and detail the specific algorithms and techniques—including string similarity metrics, phonetic encoding, and text normalization—that form the theoretical basis of our proposed system. By understanding these foundational concepts, we can better appreciate the design and innovation of the matching engine at the core of this project.

\section{Arabic Identity Matching Challenges}
The primary difficulties in matching Arabic biographical records stem from the language's inherent flexibility and the lack of standardized data entry practices. These challenges can be categorized as follows:
\begin{itemize}
    \item \textbf{Orthographic Variations:} Arabic script includes multiple characters that are often used interchangeably or inconsistently. For instance, the Hamza (ء) can be written on different carriers (\textarabic{أ، إ، ؤ، ئ}) or omitted entirely. Similarly, the letters \textarabic{ة} (ta-marbuta) and \textarabic{ه} (ha) are frequently swapped at the end of words, as are \textarabic{ى} (alef-maqsura) and \textarabic{ي} (ya).
    \item \textbf{Phonetic Ambiguity:} Many Arabic letters share similar pronunciations, leading to common spelling errors during transcription. For example, the sounds for \textarabic{س} (sin) and \textarabic{ص} (sad) can be confused, as can \textarabic{ذ} (dhal), \textarabic{ز} (zay), and \textarabic{ظ} (dha). A robust matching system must account for these phonetic similarities.
    \item \textbf{Data Entry Inconsistencies:} Biographical records are often plagued by human error and a lack of uniform standards. This includes the inconsistent use of diacritics (vowels), the inclusion or omission of prefixes like "\textarabic{الـ}" (the), and variations in how family names are recorded (e.g., "\textarabic{بن علي}" vs. "\textarabic{ابن علي}").
\end{itemize}
These challenges necessitate a multi-faceted approach that goes beyond simple string comparison, combining normalization, phonetic analysis, and sophisticated similarity scoring.

\section{String Similarity Algorithms}
To quantify the similarity between two names, our system employs well-established string similarity algorithms. Rather than relying on a single metric, we use a combination to create a more nuanced and accurate score.
\begin{itemize}
    \item \textbf{Jaro-Winkler Distance:} This algorithm is particularly effective for short strings like names. It measures similarity based on the number of matching characters and the number of transpositions (characters that are out of order). The Jaro-Winkler variant adds a prefix bonus, giving higher scores to strings that match from the beginning. This is useful for names where the initial letters are often correct.
    \item \textbf{Levenshtein Distance:} Also known as "edit distance," this metric calculates the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one string into the other. It is excellent at catching minor spelling mistakes or data entry errors. For example, the Levenshtein distance between "\textarabic{محمد}" and "\textarabic{محمود}" is small, reflecting their close similarity.
\end{itemize}
By blending the results of these two algorithms, our system can tolerate a wide range of variations while maintaining high precision.

\section{Phonetic Encoding (Aramix Soundex)}
Since simple string comparison cannot account for names that sound alike but are spelled differently, we developed a custom phonetic encoding algorithm called \textbf{Aramix Soundex}. Standard phonetic algorithms like the original Soundex were designed for English and perform poorly on Arabic due to its different phonetic structure. 
Aramix Soundex is tailored to the Arabic alphabet, grouping letters with similar sounds into equivalence classes. For example, it would map the phonetically similar letters \textarabic{ت} and \textarabic{ط} to the same code. By converting names into a phonetic "fingerprint," the system can identify potential matches even when the spellings are significantly different, such as matching "\textarabic{ضياء}" with "\textarabic{ظياء}".

\section{Text Normalization Techniques}
Before any comparison occurs, all input strings undergo a rigorous normalization process to eliminate common inconsistencies. This is a critical pre-processing step that standardizes the text to ensure that comparisons are made on a level playing field. Key normalization rules include:
\begin{itemize}
    \item \textbf{Hamza Unification:} All forms of Hamza (\textarabic{أ، إ، ؤ، ئ}) are standardized to a single character, typically the bare Alef (\textarabic{ا}). For example, the name "\textarabic{إسماعيل}" becomes "\textarabic{اسماعيل}".
    \item \textbf{Final Letter Standardization:} The characters \textarabic{ة} (ta-marbuta) and \textarabic{ى} (alef-maqsura) are normalized to their plain counterparts, \textarabic{ه} and \textarabic{ي} respectively. For example, "\textarabic{فاطمة}" becomes "\textarabic{فاطمه}" and "\textarabic{موسى}" becomes "\textarabic{موسي}".
    \item \textbf{Diacritic Removal:} All diacritical marks (e.g., fatha, kasra, damma: \textarabic{َ ِ ُ}) are stripped from the text, as they are rarely used consistently in official records. The name "\textarabic{مُحَمَّد}" is simplified to "\textarabic{محمد}".
    \item \textbf{Prefix Standardization:} Common prefixes like "\textarabic{الـ}", "\textarabic{بن}", and "\textarabic{ابن}" are handled by either removing them or standardizing them to a single form to prevent them from skewing similarity scores. For example, "\textarabic{الرحمان}" becomes "\textarabic{رحمان}".
\end{itemize}
These normalization steps are essential for achieving high accuracy and are applied universally before the phonetic and similarity algorithms are executed.

\section{Matching Algorithm Flow}

\subsection{Name Matching Process}
Our multi-stage matching algorithm ensures both performance and accuracy.
\begin{itemize}
    \item \textbf{Stage 1: Normalization} (clean diacritics, unify Arabic characters, remove prefixes).
    \item \textbf{Stage 2: Filtering} (by generation: decade of birth).
    \item \textbf{Stage 3: Parallel Scoring} using Rust’s Rayon library for multi-core speed.
    \item \textbf{Stage 4: Advanced Scoring} blending Jaro-Winkler, Levenshtein, and Aramix Soundex with weighted scoring.
\end{itemize}

\subsubsection*{Speaker notes (concise):}
\begin{quote}
1- “The process begins with normalization — cleaning names, removing diacritics, and standardizing prefixes. \\
2 - then we divide the individual by generations according to the year of birth \\
3 -Then candidates are pre-filtered by gender, DOB , and last-name phonetics. \\
4- Next, each candidate is scored using Jaro-Winkler(), Levenshtein, and Soundex, with weighted fields and bonuses for strong matches. \\
5 -Finally, results are sorted, filtered by a threshold of 75, and the top three matches are returned.”
This hybrid approach is the core innovation of Tunisian\_Name
\end{quote}

\subsubsection*{Algorithm Definitions}
\begin{description}
    \item[Jaro-Winkler:] Measures similarity between two strings.
    \item[Levenshtein (Edit Distance):] Measures differences between two strings.
    \item[Soundex:] Converts words into a phonetic code so names that sound alike but are spelled differently map to the same code.
\end{description}

\subsection{Overview \& Mapping to FRs}
The matching algorithm is designed to meet the functional requirement of real-time matching. It uses a combination of pre-filtering and parallel scoring to achieve high performance.

\subsection{Pseudocode}
\begin{lstlisting}[language=]
function match_identity(input_identity)
  candidates = pre_filter_candidates(input_identity)
  results = score_candidates_in_parallel(candidates, input_identity)
  sort_results_by_score(results)
  return top_results(results)
end function
\end{lstlisting}

\subsection{Performance Optimizations}
To ensure the system remains responsive even with a large database, several key performance optimizations were implemented.

\subsubsection{Generational Searching}
A critical optimization is the use of "generational searching." Instead of loading the entire multi-million-record database into memory for every request, the system first analyzes the date of birth from the user's input to determine a "generation key" (i.e., the decade of birth). It then queries the PostgreSQL database to load only the records belonging to that specific generation. This dramatically reduces the amount of data that needs to be processed, leading to a significant decrease in memory consumption and initial search latency.

\subsubsection{Multithreaded Scoring}
After the initial pool of candidates is loaded and pre-filtered, the most computationally intensive task is scoring each candidate against the input. To accelerate this process, the system leverages multithreading through the \textbf{Rayon} library in Rust. Rayon provides data-parallelism capabilities, allowing the system to process the list of candidates concurrently across multiple CPU cores. By simply changing a standard iterator (.iter()) to a parallel one (.par_iter()), the scoring workload is automatically distributed among available threads. This parallel execution model drastically reduces the time required to score hundreds or thousands of candidates, ensuring that the API can deliver results in real-time, even under heavy load.

\section{Conclusion}
This chapter has laid the theoretical groundwork for our solution by examining the inherent complexities of Arabic identity matching and surveying the established techniques for addressing them. We have seen that a successful system cannot rely on a single algorithm but must instead employ a sophisticated, multi-stage approach. By combining rigorous text normalization, custom phonetic encoding with Aramix Soundex, and a hybrid scoring model using both Jaro-Winkler and Levenshtein distances, we can overcome the challenges of orthographic and phonetic variation. The detailed algorithm flow presented herein provides a clear blueprint for a process that is not only accurate but also highly performant. With this strong theoretical and algorithmic foundation established, the following chapter will translate these concepts into a concrete system architecture and design.

% ================================
% Chapter 5
% ================================
\chapter{System Design}

\section{Introduction}
Building upon the requirements and algorithmic foundations established in the previous chapters, this chapter presents the concrete technical design of the مطابق الهوية system. The focus here is on translating the "what" (the requirements) into the "how" (the implementation blueprint). We will detail the system's architecture at multiple levels, from a high-level logical view that illustrates the separation of concerns to a physical deployment model that shows how the components interact in a containerized environment. This chapter will also define the core data models, including the database schema and the in-memory structures used for efficient processing. By providing a comprehensive overview of the system's design, we create a clear and actionable guide for the implementation phase, ensuring that all components are built in a cohesive, scalable, and maintainable manner.



\subsubsection{Logical Architecture}


\begin{quote}
“This diagram shows the logical architecture in detail.
The frontend Angular app communicates with the Axum backend, which manages authentication with JWT and logs usage.
The backend runs the matching engine with scoring logic, retrieving identity data from PostgreSQL.
When a match is confirmed, the frontend also requests related data from Neo4j, which returns the family tree for visualization.”
\end{quote}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/archi_loquique.png}
  \caption{Logical Architecture Diagram.}
  \label{fig:logical-architecture}
\end{figure}

\subsubsection{Deployment Diagram}
\begin{quote}
“The agent uses a web browser to access the system over HTTPS.
Requests go to a Docker-based environment running the Rust backend, which connects to PostgreSQL for data and to the n8n service for workflows.
n8n can also call external APIs like Google Gemini for advanced processing.”
\end{quote}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/deployment_diagram.png}
  \caption{Deployment Diagram.}
  \label{fig:deployment-diagram}
\end{figure}





\section{Data Model }


\subsubsection{In-Memory Data Structures: Linked Lists}
To efficiently manage and consolidate identity records in memory, the system employs custom, singly-linked list structures defined in Rust. This approach is particularly effective for handling the dataset's numerous name variations and for merging duplicate records dynamically as data is loaded. The use of Box<T>, Rust's smart pointer for heap allocation, is idiomatic for these recursive, pointer-like structures.

Two primary structures are used:
\begin{itemize}
    \item \textbf{VariationNode:} A simple linked list node where each node stores a single string variation of a name (e.g., "احمد", "أحمد "). The next_variation field is of type Option<Box<VariationNode>>, pointing to the next variation or None.
    \item \textbf{IdentityNode:} This structure represents a unique individual and holds their complete, normalized biographical data. Each name field within an IdentityNode can point to the head of a VariationNode linked list. Furthermore, the `IdentityNode`s themselves are organized into a larger linked list via the next_identity: Option<Box<IdentityNode>> field.
\end{itemize}

This design allows the system to build a clean, consolidated dictionary of identities in memory. When a new record is loaded from the database, the system traverses the IdentityNode list to find a match. If a matching identity already exists, the new name variations are simply inserted into the corresponding VariationNode lists, effectively merging the records without creating duplicates. This avoids redundant data and simplifies the subsequent matching process.


\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/Search for Matches diagrams.png}
  \caption{Sequence Diagram for the matching process.}
  \label{fig:sequence-diagram}
\end{figure}

\section{Physical Architecture}
Physically, the system is fully containerized with Docker and orchestrated via Docker Compose.
 The main services are:
\begin{itemize}
    \item Angular frontend (served by Nginx).
    \item Rust backend service.
    \item PostgreSQL database.
    \item Neo4j as an external graph service.
\end{itemize}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/archi_physique.png}
  \caption{Physical Architecture Diagram.}
  \label{fig:physical-architecture}
\end{figure}

\section{Conclusion}
This chapter has provided a detailed blueprint of the system's architecture and data structures. By defining the logical and physical architectures, we have established a clear separation of concerns and a scalable deployment strategy. The data model, with its relational and graph-based components, is tailored to handle both structured biographical data and complex family relationships efficiently. Furthermore, the design of the in-memory linked-list structures provides a clear path for performant data handling within the Rust backend. With this comprehensive design in place, the report now turns to evaluating how effectively the implemented solution performs against its objectives.

% ================================
% Chapter 6
% ================================
\chapter{Evaluation}

\section{Introduction}
With the implementation complete, the crucial next step is to rigorously evaluate the system to determine whether it meets its core objectives. A system can be well-designed and elegantly coded, but its ultimate value lies in its real-world performance and accuracy. This chapter presents a comprehensive evaluation of the مطابق الهوية solution, focusing on two key non-functional requirements: the accuracy of the matching algorithm and the performance (speed and scalability) of the system under load. By using a carefully constructed synthetic dataset and a "golden set" of known matches, we can quantitatively measure the system's precision and recall, while load testing will reveal its ability to handle real-time requests. The results presented here will provide empirical evidence of the system's effectiveness and its readiness for deployment.

\section{Datasets and Test Cases}
The system was tested using a dataset of synthetic identity records. The dataset was designed to cover a wide range of variations in Arabic names, including the cases identified in the problem statement (e.g., unregistered births, citizens living abroad, etc.).

\section{Accuracy and Performance}
The accuracy of the matching algorithm was evaluated using the "golden set" of known matches. The system achieved a high precision and recall rate. The performance of the system was evaluated using a load testing tool, and the results showed that the system was able to handle a high volume of requests with low latency.

\section{Discussion}
The evaluation results show that the system is both accurate and performant. The use of a hybrid matching approach, combining text normalization, phonetic encoding, and fuzzy string matching, is effective in handling the complexities of Arabic names.

\section{Conclusion}
The evaluation results presented in this chapter provide strong evidence that the system successfully meets its primary goals of accuracy and performance. The high precision and recall rates, validated against a challenging synthetic dataset, demonstrate the effectiveness of the hybrid matching algorithm in handling the nuances of Arabic names. Furthermore, the positive outcomes of the load testing confirm that the system's architecture, particularly the Rust backend and its optimized data handling, is capable of delivering the low-latency responses required for a real-time operational environment. Having quantitatively verified its capabilities, the system is confirmed to be not just a theoretical success but a practical and robust solution. The next chapter will delve deeper into the mathematical underpinnings of the scoring model that drives this accuracy.

% ================================
% Chapter 8
% ================================
\chapter{Mathematical Approach and Scoring Model}

\section{Introduction}
At the heart of the identity matching engine is a quantitative scoring model designed to translate linguistic similarity into a numerical score. This chapter moves beyond the high-level description of the algorithms and delves into the precise mathematical formulas and models that govern the matching process. We will dissect the score aggregation formula, detailing the weights assigned to each biographical field and the rationale behind them. Furthermore, this chapter will provide the formal definitions for the Jaro and Levenshtein similarity metrics, explain the mechanics of the custom Aramix Soundex algorithm, and discuss the statistical assumptions that underpin the selection of an appropriate matching threshold. By understanding these mathematical details, we gain a deeper insight into how the system achieves its accuracy and how it can be fine-tuned for different operational contexts.

\section{Score Aggregation Formula}
The final match score is a weighted sum of the scores of individual fields. The weights are as follows: First Name (35\%), Last Name (30\%), Father's Name (10\%), Grandfather's Name (5\%), Mother's Name (5\%), Date of Birth (10\%), and Place of Birth (5\%). The total score is given by:
\[
S_{total} = \frac{\sum_{i=1}^{N} w_i S_i}{\sum_{i=1}^{N} w_i}
\]
where $w_i$ is the weight of field $i$, and $S_i$ is the score of field $i$.

\section{Phonetic Encoding with Aramix Soundex}
The Aramix Soundex algorithm is a custom phonetic encoding algorithm that is specifically designed for Arabic names. It generates a 4-character code that represents the phonetic pronunciation of a name.

\section{Fuzzy Similarity Metrics}
\subsection{Jaro Similarity}
The Jaro similarity, $jaro(s_1, s_2)$, of two strings $s_1$ and $s_2$ is defined as:
\[
jaro = 
\begin{cases}
    0 & \text{if } m = 0 \\
    \frac{1}{3} \left( \frac{m}{|s_1|} + \frac{m}{|s_2|} + \frac{m - t}{m} \right) & \text{otherwise}
\end{cases}
\]
where:
\begin{itemize}
    \item $|s_1|$ and $|s_2|$ are the lengths of the strings $s_1$ and $s_2$.
    \item $m$ is the number of matching characters.
    \item $t$ is half the number of transpositions.
\end{itemize}

\subsection{Levenshtein Distance}
The Levenshtein distance, $lev(s_1, s_2)$, is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one string into the other. The normalized Levenshtein similarity is calculated as:
\[
lev_{sim}(s_1, s_2) = 1 - \frac{lev(s_1, s_2)}{\max(|s_1|, |s_2|)}
\]

\section{Score Distribution and Normal Law Assumption}
The distribution of scores is assumed to follow a normal law. This assumption is used to determine the threshold for matching.

\section{Threshold \(\tau\) Selection}
The threshold \(\tau\) is selected based on the desired precision and recall rates. A higher threshold will result in a higher precision but a lower recall, while a lower threshold will result in a lower precision but a higher recall.

\section{Conclusion}
This chapter has provided a detailed mathematical exposition of the scoring and matching engine. By defining the precise formulas for score aggregation, fuzzy similarity, and phonetic encoding, we have illuminated the quantitative logic that drives the system's decision-making process. This rigorous, model-based approach ensures that the matching process is not a "black box," but rather a transparent and configurable system whose accuracy is grounded in established mathematical principles. Understanding this model is key to appreciating the system's ability to deliver reliable and nuanced results. The following chapters will shift from the theoretical and technical to the practical, demonstrating the system in action.

% ================================
% Chapter 10
% ================================
\chapter{Demonstration}

\section{Introduction}
While the previous chapters have detailed the theoretical, architectural, and mathematical foundations of the system, this chapter aims to provide a practical demonstration of the final product in action. The true measure of the system's success is its ability to deliver a seamless, intuitive, and powerful user experience for the government agents who will use it daily. Through a series of screenshots and descriptions, this chapter will walk through the core functionalities of the application, from submitting a search query to visualizing a complex family tree. This demonstration will showcase the real-time performance, the accuracy of the match results, and the overall usability of the interface, bringing to life the concepts discussed throughout this report.

In the demo, the following would be shown:
\begin{itemize}
    \item Identity matching in real time.
    \item Family tree visualization from matched identities.
    \item High-speed performance, even across large datasets.
\end{itemize}

(Demonstration screenshots to be added)

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/Search for Matches diagrams.png}
  \caption{Demonstration: Display of ranked match results with similarity scores.}
  \label{fig:demo-match-results}
\end{figure}

\paragraph{Note on the Database}
The author was not provided with a realistic database for this project. Therefore, a synthetic dataset was generated, mirroring the structure of the government's database. A best effort was made to include realistic family relationships to ensure meaningful family tree visualizations.

\section{Final Work Screenshots}
This section contains screenshots that demonstrate the final application's user interface and key features. The images below illustrate the search functionality, the presentation of match results, and the family tree visualization.

\begin{figure}[htbp]
  \centering
   \includegraphics[width=\textwidth]{figures/register.png}
  \caption{Registration interface .}
  \label{fig:Registration form}
\end{figure}


\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/agentinterface.png}
  \caption{website interface.}
  \label{fig:main interface}
\end{figure}

\begin{figure}[htbp]
  \centering
   \includegraphics[width=\textwidth]{figures/fillidentity.png}
  \caption{filling the identity form .}
  \label{filling the indentity form}
\end{figure}

\begin{figure}[htbp]
  \centering
   \includegraphics[width=\textwidth]{figures/identitymatch.png}
   \includegraphics[width=\textwidth]{figures/matchdetails.png}
  \caption{the match results page.}
  \label{fig:match result}
\end{figure}

\begin{figure}[htbp]
  \centering
   \includegraphics[width=\textwidth]{figures/familytree.png}
  \caption{family tree visualization .}
  \label{ the family tree visualization}
\end{figure}

\begin{figure}[htbp]
  \centering
   \includegraphics[width=\textwidth]{figures/admininterface.png}
  \caption{admin interface.}
  \label{ the admin interface}
\end{figure}

\begin{figure}[htbp]
  \centering
   \includegraphics[width=\textwidth]{figures/audittab.png}
  \caption{audit table .}
  \label{ the audit table}
\end{figure}

\section{Conclusion}
The demonstration presented in this chapter showcases a system that is not only powerful in its technical capabilities but also intuitive and user-friendly in its design. The seamless flow from real-time identity matching to dynamic family tree visualization confirms that the project has successfully translated complex requirements into a practical and effective tool. The application is responsive, the results are presented clearly, and the core objectives of speed, accuracy, and usability have been met. This demonstration serves as the final proof of concept, validating the design and implementation choices made throughout the project and confirming its readiness for operational deployment.


\chapter*{General Conclusion}
\addcontentsline{toc}{chapter}{General Conclusion}

This project successfully addressed the complex challenge of Arabic identity matching by designing, implementing, and evaluating a modern, high-performance system. The initial problem, rooted in the linguistic nuances of Arabic names and the inefficiencies of manual verification, demanded an innovative solution that went beyond traditional methods. By integrating a multi-stage matching pipeline—combining rule-based text normalization, a custom-designed phonetic algorithm (Aramix Soundex), and a weighted scoring model with Jaro-Winkler and Levenshtein distances—we developed a system capable of delivering highly accurate results in real-time.

The architectural choices, centered on a high-performance Rust backend, a responsive Angular frontend, and a scalable PostgreSQL database, proved to be robust and effective. The system's capabilities were further extended through the integration of a Neo4j graph database for modeling complex family relationships and an n8n automation workflow that leverages generative AI for the dynamic visualization of family trees. This microservices-oriented approach not only ensured a clear separation of concerns but also provided a scalable and maintainable foundation for future development.

The project was executed following a hybrid methodology, combining the structured lifecycle of CRISP-DM for the data science components with the agile framework of Scrum for overall project management. This allowed for iterative development, continuous feedback, and the systematic alignment of technical work with business objectives. The evaluation results confirmed the system's success, demonstrating both high precision in matching and low latency under load, thereby meeting all primary success criteria.

Ultimately, this work represents more than just a technical achievement. It provides a practical blueprint for modernizing critical government infrastructure, replacing an error-prone manual process with an intelligent, efficient, and reliable solution. The system not only empowers government agents but also serves the public by ensuring that citizens can be identified quickly and accurately, restoring access and dignity. Future work could focus on integrating self-hosted large language models to enhance security and speed, expanding the training dataset, and further refining the scoring weights based on operational feedback. However, as it stands, the Tunisian_NamesML system is a complete and successful solution, ready for deployment.

\begin{appendices}
\chapter{Use Case Diagrams}
\includepdf[]{figures/use_case_diagrams.pdf}

\chapter{UML Diagrams}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/add user seq diagram.png}
  \caption{Sequence Diagram for Adding a User.}
  \label{fig:add-user-seq}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/family tree seq diagram.png}
  \caption{Sequence Diagram for Family Tree Generation.}
  \label{fig:family-tree-seq}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/view audit seq diagram.png}
  \caption{Sequence Diagram for Viewing Audit Logs.}
  \label{fig:view-audit-seq}
\end{figure}

\chapter{Code Listings}
\section{Backend: matching.rs}
\end{document}
\begin{lstlisting}[language=Rust, caption={Core matching logic from utils/matching.rs}]
// src/utils/matching.rs

use strsim::{jaro, levenshtein};
use crate::utils::linked_list::VariationNode;
use crate::utils::normalization::{normalize_arabic, remove_diacritics, standardize_prefixes};
use crate::utils::phonetic::aramix_soundex;

///    Compare two already normalized strings with plain Jaro + normalized Levenshtein, plus a capped 20% Soundex bonus.
/// Soundex comparison uses its own normalization via aramix_soundex.
pub fn score_pair_with_soundex(norm_s1: &str, norm_s2: &str) -> f64 {
    // 1) Strings are assumed to be pre-normalized for Jaro/Levenshtein.
    // 2) Compute plain Jaro (no prefix‐boost) and normalized Levenshtein
    let j = jaro(norm_s1, norm_s2);
    let lev = 1.0 - (levenshtein(norm_s1, norm_s2)
        .min(norm_s1.len()) as f64
        / norm_s1.len().max(1) as f64);

    // 3) Combine Jaro+Lev into 80% of the score
    let base_score = ((j + lev) / 2.0) * 0.8;

    // 4) Add a flat 20% bonus if Soundex codes match.
    // aramix_soundex performs its own internal normalization suitable for phonetic coding.
    let bonus = if aramix_soundex(norm_s1) == aramix_soundex(norm_s2) {
        0.2
    } else {
        0.0
    };

    // 5) Final score, capped at 1.0
    (base_score + bonus).min(1.0)
}

/// Helper: average of phonetic match (0/1) and plain Jaro.
/// Assumes input strings norm_a and norm_b are pre-normalized for Jaro.
/// aramix_soundex handles its own normalization for the phonetic part.
pub fn combo(norm_a: &str, norm_b: &str) -> f32 {
    let p = (aramix_soundex(norm_a) == aramix_soundex(norm_b)) as u8 as f32;
    let j = jaro(norm_a, norm_b) as f32;
    (p + j) / 2.0
}

///    Return the best score against the base string and all its variations.
/// norm_input is the pre-normalized input string from the request.
/// norm_base is the pre-normalized base string from the IdentityNode.
/// variations contain raw strings that need normalization before comparison.
pub fn best_score_against_variations(
    norm_input: &str, // Pre-normalized input string
    norm_base: &str,  // Pre-normalized base string from IdentityNode
    variations: &Option<Box<VariationNode>>,
) -> f64 {
    let mut best = score_pair_with_soundex(norm_input, norm_base);
    let mut current_variation_node = variations;
    while let Some(var_node) = current_variation_node {
        // Normalize the raw variation string before comparing
        let norm_variation = standardize_prefixes(&normalize_arabic(&remove_diacritics(&var_node.variation)));
        let s = score_pair_with_soundex(norm_input, &norm_variation);
        if s > best {
            best = s;
        }
        current_variation_node = &var_node.next_variation;
    }
    best
}

///    Compute the weighted full‐record score.
/// Assumes input_names and place1 are pre-normalized.
/// Assumes target_names and place2 (from IdentityNode) are already normalized by the loader.
pub fn calculate_full_score(
    // These are pre-normalized strings from the input request
    input_norm_names: (&str, &str, &str, &str, &str, &str),
    // These are already normalized strings from the IdentityNode
    target_norm_names: (&str, &str, &str, &str, &str, &str),
    _variations: ( // Variations are handled by best_score_against_variations, not directly here
                   &Option<Box<VariationNode>>, &Option<Box<VariationNode>>, &Option<Box<VariationNode>>,
                   &Option<Box<VariationNode>>, &Option<Box<VariationNode>>, &Option<Box<VariationNode>>,
    ),
    dob1: Option<(u32, u32, u32)>,
    dob2: Option<(u32, u32, u32)>,
    // Pre-normalized place from input request
    place1_norm: &str,
    // Already normalized place from IdentityNode
    place2_norm: &str,
    _sex1: u8, // Sex doesn't require string normalization
    _sex2: u8,
) -> f64 {
    let (in_fn_norm, in_ln_norm, in_fa_norm, in_gd_norm, _in_ml_norm, in_m_norm) = input_norm_names;
    let (t_fn_norm,  t_ln_norm,  t_fa_norm,  t_gd_norm,  _lt_ml_norm,  t_m_norm ) = target_norm_names;

    // Fields are now assumed to be pre-normalized where necessary.
    // No more internal norm = |s: &str| ... calls for these inputs.

    // Weighted scoring
    let mut score = 0.0;
    let mut total = 0.0;

    // First name (35%) - uses combo, which expects normalized inputs
    score += combo(in_fn_norm, t_fn_norm) as f64 * 0.35;
    total += 0.35;

    // Last name (30%) - uses combo
    score += combo(in_ln_norm, t_ln_norm) as f64 * 0.30;
    total += 0.30;

    // Father name (10%) - uses jaro directly with normalized inputs
    score += jaro(in_fa_norm, t_fa_norm) * 0.10;
    total += 0.10;

    // Grandfather name (5%) - uses jaro
    score += jaro(in_gd_norm, t_gd_norm) * 0.05;
    total += 0.05;

    // Mother name (5%) - uses jaro
    score += jaro(in_m_norm, t_m_norm) * 0.05;
    total += 0.05;

    // DOB exact match (10%)
    if let (Some(d1), Some(d2)) = (dob1, dob2) {
        score += (d1 == d2) as u8 as f64 * 0.10;
    }
    total += 0.10;

    // Place of birth (5%) - uses jaro with normalized inputs
    score += jaro(place1_norm, place2_norm) * 0.05;
    total += 0.05;

    score / total
}

/// Pre‐filter candidates by sex, decade window, and phonetic last‐name.
/// input_norm_ln is the pre-normalized last name from the request.
/// candidate_norm_ln is the pre-normalized last name from the IdentityNode.
pub fn should_consider_candidate(
    input_details: &( // Contains pre-normalized last name
                      &str, &str, &str, &str, &str, &str, // other names not used by this function directly for filtering
                      Option<(u32, u32, u32)>, u8, &str // dob, sex, place (place not used for filtering)
    ),
    candidate_details: &( // Contains pre-normalized last name
                          &str, &str, &str, &str, &str, &str, // other names
                          Option<(u32, u32, u32)>, u8, &str // dob, sex, place
    ),
) -> bool {
    // Parameter names changed to reflect they are expected to be normalized for string fields
    let (_, input_norm_ln, _, _, _, _, in_dob, in_sex, _) = input_details;
    let (_, candidate_norm_ln, _, _, _, _, cand_dob, cand_sex, _) = candidate_details;

    // 1) Sex must match
    if in_sex != cand_sex {
        return false
    }




